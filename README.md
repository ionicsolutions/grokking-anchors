# Grokking Anchors: Uncovering What a Machine-Learning Model Relies On

Kilian Kluge @ PyConDE & PyData Berlin 2023, [April 19th 2023, 11:50, A1](https://2023.pycon.de/program/QUAXG3/)

# Links & further reading
- Ribeiro et al. (2018): [Anchors: High-Precision Model-Agnostic Explanations](https://ojs.aaai.org/index.php/AAAI/article/view/11491/11350)
- Goerke & Lang: [Scoped Rules (Anchors)](https://christophm.github.io/interpretable-ml-book/anchors.html)
  (part of Christopher Molnar: _Interpretable Machine Learning_)
- Kaufmann,& Kalyanakrishnan (2013): [Information Complexity in Bandit Subset Selection](http://proceedings.mlr.press/v30/Kaufmann13.html)
- Kluge & Eckhardt (2020): [Explaining Suspected Phishing Attempts with Document Anchors](Kluge%202020%20-%20Explaining%20Suspected%20Phishing%20Attempts%20with%20Document%20Anchors.pdf) 2020 ICML Workshop on Human Interpretability in Machine
Learning (WHI 2020)
- Anchor implementations:
  - [Original Anchors on GitHub](https://github.com/marcotcr/anchors) & [package on PyPI](https://pypi.org/project/anchors)
  - [Documentation for Anchors in `alibi`](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html) & [package on PyPI](https://pypi.org/project/alibi/)
- Affiliations:
  - [inlinity](https://www.inlinity.ai) ([LinkedIn](https://www.linkedin.com/company/inlinity/))
  - [XAI Studio](https://www.xai-studio.de) ([LinkedIn](https://www.linkedin.com/company/xai-studio/))
